{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3351fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /Users/jennyhe/miniconda3/envs/llm/lib/python3.11/site-packages (2.6.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/jennyhe/miniconda3/envs/llm/lib/python3.11/site-packages (from openai) (4.11.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/jennyhe/miniconda3/envs/llm/lib/python3.11/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/jennyhe/miniconda3/envs/llm/lib/python3.11/site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in /Users/jennyhe/miniconda3/envs/llm/lib/python3.11/site-packages (from openai) (0.11.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/jennyhe/miniconda3/envs/llm/lib/python3.11/site-packages (from openai) (2.12.3)\n",
      "Requirement already satisfied: sniffio in /Users/jennyhe/miniconda3/envs/llm/lib/python3.11/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /Users/jennyhe/miniconda3/envs/llm/lib/python3.11/site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /Users/jennyhe/miniconda3/envs/llm/lib/python3.11/site-packages (from openai) (4.15.0)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/jennyhe/miniconda3/envs/llm/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
      "Requirement already satisfied: certifi in /Users/jennyhe/miniconda3/envs/llm/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/jennyhe/miniconda3/envs/llm/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/jennyhe/miniconda3/envs/llm/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/jennyhe/miniconda3/envs/llm/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in /Users/jennyhe/miniconda3/envs/llm/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (2.41.4)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /Users/jennyhe/miniconda3/envs/llm/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Python executable: /Users/jennyhe/miniconda3/envs/llm/bin/python\n",
      "OpenAI installation check:\n",
      "✓ OpenAI successfully imported\n",
      "OpenAI version: 2.6.1\n"
     ]
    }
   ],
   "source": [
    "%pip install openai\n",
    "import sys\n",
    "print(\"Python executable:\", sys.executable)\n",
    "print(\"OpenAI installation check:\")\n",
    "try:\n",
    "    import openai\n",
    "    print(\"✓ OpenAI successfully imported\")\n",
    "    print(\"OpenAI version:\", openai.__version__)\n",
    "except ImportError as e:\n",
    "    print(\"✗ OpenAI import failed:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2becc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# openai.api_key = \"sk-proj-1234567890\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16655bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0,\n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229ba536",
   "metadata": {},
   "source": [
    "### Two Principles for Prompting:\n",
    "\n",
    "1. **Write clear and specific instructions**\n",
    "     - Tactic 1: Use delimiters, such as:\n",
    "       - Triple quotes: \"\"\"\n",
    "       - Triple backticks: ```\n",
    "       - Triple dashes: ---\n",
    "       - Angle brackets: < >\n",
    "       - XML tags: <tag> </tag>\n",
    "     - Tactic 2: Ask for structured output\n",
    "       - HTML, JSON\n",
    "     - Tactic 3: Check whether conditions are satisfied\n",
    "       - check assumptions required to do the task\n",
    "     - Tactic 4: Few-shot prompting\n",
    "       - Give successful examples of completing tasks, then ask model to perform the task\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5780d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example for tactic 1\n",
    "text = f\"\"\"\n",
    "You should express what you want a model to do by providing instructions that are clear and specific as you can possibly make them. This will guide the model towards the desired output, and reduce the chances of receiving an irrelevant or incorrect response. Don't confuse writing a clear prompt with writing a short prompt. In many cases, longer prompts provide more clarity and context for the model, which can lead to more detailed and relevant outputs.\n",
    "\"\"\"\n",
    "prompt = f\"\"\"\n",
    "Summarize the text delimited by triple backticks into a single sentence.\n",
    "```{text}```\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d646db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example for tactic 2\n",
    "prompt = f\"\"\" \n",
    "Generate a list of three made-up book titles along with their authors and genre.\n",
    "Provide them in a JSON format with the following keys: book_id, title, author, genre.\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c089124b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example for tactic 3\n",
    "text = f\"\"\"\n",
    "Making a cup of tea is easy! First, you need to get some water boiling. While that's happening, grab a cup and put a tea bag in it. Once the water is hot enough, turn off the heat and pour the water over the tea bag. Let it sit for a bit so the tea can brew. After a few minutes, take out the tea bag. If you like, you can add some sugar or milk to taste. And that's it! You've got yourself a delicious cup of tea to enjoy.\n",
    "\"\"\"\n",
    "prompt = f\"\"\"\n",
    "You will be provided with a text delimited by triple quotes.\n",
    "If it contains a sequence of instructions, rewrite those instructions in the following format:\n",
    "\n",
    "Step 1 - ...\n",
    "Step 2 - ...\n",
    "...\n",
    "Step N - ...\n",
    "\n",
    "If the text does not contain a sequence of instructions, just output \"No instructions found\".\n",
    "\n",
    "\\\"\\\"\\\"\n",
    "{text}\n",
    "\\\"\\\"\\\"\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(\"Compeletion for text:\" )\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb4a724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example for tactic 3 - continuation\n",
    "text = f\"\"\"\n",
    "The sun shines brightly in a cloudless blue sky, while a gentle breeze carries the light scent of blooming flowers. The warmth of the day is perfectly balanced by the soft wind, making it a delightful and peaceful atmosphere. Everything feels calm and full of promise under this beautiful, fair-weather day.\n",
    "\"\"\"\n",
    "prompt = f\"\"\"\n",
    "You will be provided with a text delimited by triple quotes.\n",
    "If it contains a sequence of instructions, rewrite those instructions in the following format:\n",
    "\n",
    "Step 1 - ...\n",
    "Step 2 - ...\n",
    "...\n",
    "Step N - ...\n",
    "\n",
    "If the text does not contain a sequence of instructions, just output \"No instructions found\".\n",
    "\n",
    "\\\"\\\"\\\"\n",
    "{text}\n",
    "\\\"\\\"\\\"\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(\"Compeletion for text:\" )\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da78041d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example for tactic 4\n",
    "prompt = f\"\"\"\n",
    "Your task is to answer in a consistent style.\n",
    "\n",
    "<child>: Teach me about patience.\n",
    "\n",
    "<grandparent>: The river that carves the deepest valley flows from a modest spring; the grandest symphony originates from a single note; the most intricate tapestry begins with a solitary thread.\n",
    "\n",
    "<child>: Teach me about resilience.\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1142475b",
   "metadata": {},
   "source": [
    "2. **Give the model time to think**\n",
    "    - Tactic 1: Specify the steps to complete a task\n",
    "       - Step 1:...\n",
    "       - Step 2:...\n",
    "       - ...\n",
    "       - Step N:...\n",
    "    - Tactic 2: Instruct the model to work out its own solution before rushing to a conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8a9206",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = f\"\"\"\n",
    "In a charming village, sibling Jack and Jill set out on a quest to fetch water from a hilltop well. As they climbed, singing joyfully, misfortune struck - Jack tripped on a stone and tumbled down the hill, with Jill following suit.\n",
    "Though slightly battered, the pair returned home to comforting embraces. Despite the mishap, their adventurous spirit remained undimmed, and they continued exploring with delight.\n",
    "\"\"\"\n",
    "prompt_1 = f\"\"\"\n",
    "Performing the following actions:\n",
    "1 - Summarize the following text delimited by triple brackticks with 1 sentence.\n",
    "2 - Translate the summary into Spanish.\n",
    "3 - List each name in the Spanish summary.\n",
    "4 - Output a json object with the following keys: Spanish_summary, num_names.\n",
    "\n",
    "Separate your answer with line breaks.\n",
    "Text:\n",
    "```{text}```\n",
    "\"\"\"\n",
    "response_1 = get_completion(prompt_1)\n",
    "print(\"Completion for prompt 1:\")\n",
    "print(response_1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1903d1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Determine if the student's solution is correct or not.\n",
    "\n",
    "Question:\n",
    "I'm building a solar power installation and I need \\\n",
    " help working out the financials.\n",
    "- Land costs $100 / square foot\n",
    "- I can buy solar panels for $250 / square foot\n",
    "- I negotiated a contract for maintenance that will cost \\\n",
    " me a flat $100k per year, and an additional $10 / square \\\n",
    " foot\n",
    "What is the total cost for the first year of operations\n",
    "as a function of the number of square feet.\n",
    "\n",
    "Student's Solution:\n",
    "Let x be the size of the installation in square feet.\n",
    "Costs:\n",
    "1. Land cost: 100x\n",
    "2. Solar panel cost: 250x\n",
    "3. Maintenance cost: 100,000 + 100x\n",
    "Total cost: 100x + 250x + 100,000 + 100x = 450x + 100,000\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c2aefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Your task is to determine if the student's solution is correct or not.\n",
    "To solve the problem do the following:\n",
    "- First, work out your own solution to the problem.\n",
    "- Then compare your solution to the student's solution and evaluate if the student's solution is correct or not.\n",
    "- Don't decide if the student's solution is correct until you have done the problem yourself.\n",
    "\n",
    "Use the following format:\n",
    "Question:\n",
    "---\n",
    "question here\n",
    "---\n",
    "Student's solution:\n",
    "---\n",
    "student's solution here\n",
    "---\n",
    "Actual solution:\n",
    "---\n",
    "steps to work out the solution and your solution here\n",
    "---\n",
    "Is the student's solution the same as actual solution just calculated:\n",
    "---\n",
    "yes or no\n",
    "---\n",
    "Student grade:\n",
    "---\n",
    "correct or incorrect\n",
    "---\n",
    "\n",
    "Question:\n",
    "---\n",
    "I'm building a solar power installation and I need help working out the financials.\n",
    "- Land costs $100 / square foot\n",
    "- I can buy solar panels for $250 / square foot\n",
    "- I negotiated a contract for maintenance that will cost me a flat $100k per year, and an additional $10 / square foot\n",
    "---\n",
    "\n",
    "Student's solution:\n",
    "---\n",
    "Let x be the size of the installation in square feet.\n",
    "Costs:\n",
    "1. Land cost: 100x\n",
    "2. Solar panel cost: 250x\n",
    "3. Maintenance cost: 100,000 + 100x\n",
    "Total cost: 100x + 250x + 100,000 + 100x = 450x + 100,000\n",
    "---\n",
    "Actual solution:\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b98836",
   "metadata": {},
   "source": [
    "### Model Limitations:\n",
    "\n",
    "1. ** Hallucination\n",
    "   - Makes statements that sound plausible but are not true\n",
    "  \n",
    "2. ** Reducing hallucinations:\n",
    "   - First find relevant information, then answer the question based on the relevant information."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
